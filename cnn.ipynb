{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(directory):\n",
    "\n",
    "    \n",
    "    labels = []\n",
    "    mel_specs = []\n",
    "    \n",
    "    \n",
    "    for file in os.scandir(directory):\n",
    "        \n",
    "        y, sr = librosa.core.load(file)\n",
    "        \n",
    "        label = str(file).split('.')[0][11:]\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Computing the spectrograms\n",
    "        #spec = np.abs(librosa.stft(y, hop_length=512))\n",
    "        #spec = librosa.amplitude_to_db(spec, ref=np.max) # converting to decibals\n",
    "        # If you want the mel spectrograms, use this.\n",
    "        spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "        spec = librosa.power_to_db(spec, ref=np.max) # Converting to decibals\n",
    "        # Adjusting the size to be 128 x 660\n",
    "        if spec.shape[1] != 660:\n",
    "            spec.resize(128,660, refcheck=False)\n",
    "            \n",
    "        mel_specs.append(spec)\n",
    "        \n",
    "    X = np.array(mel_specs)\n",
    "    \n",
    "    labels = pd.Series(labels)\n",
    "    label_dict = {\n",
    "        'jazz': 0,\n",
    "        'reggae': 1,\n",
    "        'rock': 2,\n",
    "        'blues': 3,\n",
    "        'hiphop': 4,\n",
    "        'country': 5,\n",
    "        'metal': 6,\n",
    "        'classical': 7,\n",
    "        'disco': 8,\n",
    "        'pop': 9\n",
    "    }\n",
    "    y = labels.map(label_dict).values\n",
    "    \n",
    "    # Returning the mel spectrograms and labels\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = extract_mel_spectrogram('../data/data')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.min()\n",
    "X_train /= -80\n",
    "X_test /= -80\n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(23456)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "cnn_model = Sequential(name='cnn_1')\n",
    "\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "cnn_model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,21), range(1,21))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = history.history['accuracy']\n",
    "test_loss = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(train_loss, label='Training Accuracy', color='blue')\n",
    "plt.plot(test_loss, label='Testing Accuracy', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Accuracy', fontsize = 18)\n",
    "plt.xticks(range(1,21), range(1,21))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions = cnn_model.predict(X_test, verbose=1)\n",
    "for i in range(10): \n",
    "    print(f'{i}: {sum([1 for target in y_test if target[i] == 1])}')\n",
    "for i in range(10): \n",
    "    print(f'{i}: {sum([1 for prediction in predictions if np.argmax(prediction) == i])}')\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df = pd.DataFrame(conf_matrix)\n",
    "# Creating a dictionary of labels\n",
    "labels_dict = {\n",
    "    0: 'jazz',\n",
    "    1: 'reggae',\n",
    "    2: 'rock',\n",
    "    3: 'blues',\n",
    "    4: 'hiphop',\n",
    "    5: 'country',\n",
    "    6: 'metal',\n",
    "    7: 'classical',\n",
    "    8: 'disco',\n",
    "    9: 'pop'\n",
    "}\n",
    "confusion_df = confusion_df.rename(columns=labels_dict)\n",
    "confusion_df.index = confusion_df.columns\n",
    "plt.figure(figsize= (20,12))\n",
    "sns.set(font_scale = 2);\n",
    "ax = sns.heatmap(confusion_df, annot=True, cmap=sns.cubehelix_palette(50));\n",
    "ax.set(xlabel='Predicted Values', ylabel='Actual Values');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
