{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "mel_specs = pd.read_csv('../data/genre_mel_specs_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_subset(data, genre_subset):\n",
    "    '''\n",
    "    This function takes in a dataframe and a list of genres and returns a new dataframe only including\n",
    "    the genres in the given list. Its index is reset and new labels are created so that the labels are 0 \n",
    "    through one less than the number of genres. \n",
    "    '''\n",
    "    \n",
    "    # Filtering the dataframe for the subset of the genres and resetting the index\n",
    "    df = data.loc[data['labels'].isin(genre_subset)]\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    # Creating a new label dictionary\n",
    "    new_label_dict = {}\n",
    "    for i in range(len(genre_subset)):\n",
    "        new_label_dict[genre_subset[i]] = i\n",
    "    \n",
    "    # Changing labels to be the new labels\n",
    "    df['y'] = df['labels'].map(new_label_dict)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mel_spec_data(data, genre_subset):\n",
    "    '''\n",
    "    This function takes in a dataframe of audio files and a list of genres,\n",
    "    calls the function get_genre_subset to get a dataframe including only the given genres,\n",
    "    and completes all of the data preprocessing steps needed to run a neural network.\n",
    "    \n",
    "    Preprecessing steps include:\n",
    "    1. Reshaping the mel spectrograms to their original form (128 x 660)\n",
    "    2. Defining the array of targets\n",
    "    3. Train test split\n",
    "    4. Standardizing the data\n",
    "    5. Reshaping the data to be 128 x 660 x 1, where the 1 represents a single color channel\n",
    "    6. One-hot-encoding target data\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): a dataframe of audio files, flattened mel spectrograms, and genre labels\n",
    "    genre_subset (list): a list of genres included in the dataframe\n",
    "    \n",
    "    Returns:\n",
    "    X_train (array): training set of features\n",
    "    X_test (array): testing set of features\n",
    "    y_train (array): training set of targets\n",
    "    y_test (array): testing set of targets\n",
    "    '''\n",
    "    \n",
    "    # Getting a subset of the genres using our genre_subset function\n",
    "    subset = get_genre_subset(data, genre_subset)\n",
    "    \n",
    "    # Dropping label columns to prepare our feature vector\n",
    "    specs = subset.drop(columns=['labels', 'y'])\n",
    "    print(len(specs))\n",
    "    # Reshaping the arrays to their original \"image\" form\n",
    "    X = []\n",
    "    for i in range(len(genre_subset)*100):\n",
    "        X.append(np.array(specs.iloc[i]).reshape(128,660))\n",
    "        \n",
    "    # Converting list X to an array\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Defining our targets\n",
    "    y = subset.loc[subset['labels'].isin(genre_subset), 'y'].values\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)\n",
    "    \n",
    "    # Scaling our data to be between 0 and 1\n",
    "    X_train /= -80\n",
    "    X_test /= -80\n",
    "    \n",
    "    # Reshaping images to be 128 x 660 x 1\n",
    "    X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)\n",
    "    \n",
    "    # One hot encoding our labels\n",
    "    y_train = to_categorical(y_train, len(genre_subset))\n",
    "    y_test = to_categorical(y_test, len(genre_subset))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the genres\n",
    "genre_list = [ \n",
    "    'jazz',\n",
    "    'reggae',\n",
    "    'rock',\n",
    "    'blues',\n",
    "    'hiphop',\n",
    "    'country',\n",
    "    'metal',\n",
    "    'classical',\n",
    "    'disco',\n",
    "    'pop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_subset = [\n",
    "    'jazz',\n",
    "    'reggae',\n",
    "    'rock',\n",
    "    'blues',\n",
    "    'hiphop',\n",
    "    'country',\n",
    "    'metal',\n",
    "    'classical',\n",
    "    'disco',\n",
    "    'pop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our function to get our features and targets\n",
    "X_train, X_test, y_train, y_test = preprocess_mel_spec_data(mel_specs, genre_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23456)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "cnn_model = Sequential(name='cnn_1')\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer for regularization\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "# Adding an output layer\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,18), range(1,18))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train accuracy and test accuracy over epochs.\n",
    "train_loss = history.history['accuracy']\n",
    "test_loss = history.history['val_accuracy']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Accuracy', color='blue')\n",
    "plt.plot(test_loss, label='Testing Accuracy', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Accuracy', fontsize = 18)\n",
    "plt.xticks(range(1,18), range(1,18))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions = cnn_model.predict(X_test, verbose=1)\n",
    "# Calculating the confusion matrix \n",
    "# row: actual\n",
    "# columns: predicted\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\n",
    "# Creating a dataframe of the confusion matrix with labels for readability \n",
    "confusion_df = pd.DataFrame(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_labels = {\n",
    "    0:'jazz',\n",
    "    1:'hiphop',\n",
    "    2:'country',\n",
    "    3:'metal',\n",
    "    4:'classical',\n",
    "    5:'disco',\n",
    "    6:'pop'\n",
    "}\n",
    "# Renaming rows and columns with labes\n",
    "confusion_df = confusion_df.rename(columns=genre_labels)\n",
    "confusion_df.index = confusion_df.columns\n",
    "confusion_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
